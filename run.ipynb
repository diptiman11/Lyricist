{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# songLLM : Instructions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains the instructions that we can follow to train an LLM (over spotify's million songs data) as well as serve it over an API so we can request it a response to a prompt by cURL command.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory consists of a train.py file, which we use to train it over the dataset.\n",
    "The dataset for this is in the directory \"/dataset/spotify_millsongdata.csv\" in a csv format. We use only the text column from the csv file to train our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to run the train the file, and give the parameters of the file path used for the dataset in the 'fp' argument and the learning rate as 'lr'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this command to run the train file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --fp dataset/spotify_millsongdata.csv --lr 5e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our file would start training now, and the weights and configuration file would be saved in the /out/songLLM directory. We can move forward to doing inference on the model using the server.py file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the server using this command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m uvicorn server:app --reload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have run our server, we can request an output from it using the curl command."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would get a json file in return which would consist of our generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set json={\"text\": \"End of Horizon\", \"max_length\": 1000}\n",
    "!curl -s -X POST -H \"Content-Type: application/json\" -d \"$json\" http://127.0.0.1:8000/generatetext | jq -r '.generated_text' > output.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Horizon  \n",
    "I got to get her water  \n",
    "I got to go nows  \n",
    "Cause the streets on a rock  \n",
    "Will you bang at all  \n",
    "And tell me  \n",
    "But I need you  \n",
    "And they say  \n",
    "Well  \n",
    "You don't know how  \n",
    "Will you rock?? 'nt kissin' 'n' 'n 'c??  \n",
    "Check her love me  \n",
    "So fine, check you  \n",
    "  \n",
    "Will you see me?  \n",
    "I'm gonna see the message?\"  \n",
    "I?ll get over an earth.  \n",
    "I love you  \n",
    "And I know how to go  \n",
    "I know what to go 'til ya' , I'love you , and you'll get me don't you know  \n",
    "I'm in love alone alone  \n",
    "No, , I see you run alone  \n",
    "  \n",
    "You know, you know.  \n",
    "It's just how  \n",
    "It's how,  \n",
    "No, I don't everything I lonely is better..\n",
    "\n",
    "\n",
    "It's mattique to ear you  \n",
    "It's matter I rippeD  \n",
    "It's\n",
    "\n",
    "\n",
    "It's hard to eat you and know how much no other way  \n",
    "To eat you and know how much, oh, oh, oh, oh  \n",
    "It does real night, oh, oh , oh alright, oh, oh  \n",
    "Oh, oh no, oh, oh  \n",
    "  \n",
    "Can take it all away  \n",
    "Take it all my lightning like it all and wonder it'll see, eath  \n",
    "Take it all away  \n",
    "I know it all away  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comming to the part of using multithreading for stress testing as well as making our model work fast (using CLI).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stress.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach for the stress test part was to use multithreading to create some number of threads and each thread with some number of junk requests, these parameters can be given as a input by CLI easily. After giving the input of threads as well as requests, the program bombards the given url with requests. \n",
    "\n",
    "I successfully bottlenecked the server with about 10 threads and 200 requests which means I recieved no requests for over 5-10 minutes for such a input.\n",
    "\n",
    "This command can be used to stress thread the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python stress_test.py --url http://127.0.0.1:8000/generatetext --threads 10 --requests 10 --max_length 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal_inference.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file I used argparse to take inputs of the requests (multiple at same time) and send those requests one by one to the server.\n",
    "I sent these requests to the server one by one: \"I\" \"have\" \"applied\" \"for\" \"NimbleBox\" \"Internship\".\n",
    "I got the response in about 39 seconds.\n",
    "The max throughput from the system (rps) was around 6.5 seconds.\n",
    "We can do normal inference using this particular command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python normal_inference.py --url http://127.0.0.1:8000/generatetext --messages \"I\" \"have\" \"applied\" \"for\" \"2WINs\" \"Internship\" --max_length 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi_inference.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file I wrapped multiple requests into the same number of threads and bombarded the server with requsts at the same time, and was able to achieve a much higher throughput (rps) of around 4.5 seconds.\n",
    "We can do multi inference using this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python multi_inference.py --url http://127.0.0.1:8000/generatetext --messages \"I\" \"have\" \"applied\" \"for\" \"2WINs\" \"Internship\" --max_length 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to say that I learned a lot in this particular assignment/challenge, such that I didn't knew to create APIs before this, nor did I used them a lot. However, for this particular assignment I was not only able to create one API but was also able to deploy a server, send requests to it, and also was able to do inference (multithreaded also) from the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
